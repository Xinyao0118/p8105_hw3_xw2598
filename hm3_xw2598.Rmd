---
title: "p8105_hw3_xw2598"
author: Xinyao Wu
date: 2018-10-05
output: github_document
---

#problem1

```{r p1_dataset}
#build a dataset
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
library(tidyverse)
library(ggridges)
data(brfss_smart2010)
brfss = brfss_smart2010 %>% 
#data cleaning
#rename variables
  janitor::clean_names() %>% 
#focus on the “Overall Health” topic
  filter(topic == "Overall Health") %>% 
#include only responses from “Excellent” to “Poor”  (1)???
  select(year,locationabbr,locationdesc,response,data_value) %>% 
#organize responses as a factor
  mutate(
    response = factor(response)
  )
head(brfss, 10)
```


```{r loc_count}
#In 2002, which states were observed at 7 locations
loc_brfss = brfss %>% 
  group_by(year,locationabbr) %>% 
  summarise(
    count = n_distinct(locationdesc)
    )
  filter(loc_brfss,count == 7,year == 2002)
```
Comments: In 2002,CT,FL,NC were observed at 7 locations. I choose distinct specific locations  which were observed in each state as the dataset, because from the original data each specific location were repetitive and the repetity doesn`t make sence in this question.

```{r spaghetti}
#spaghetti plot
ggplot(loc_brfss, aes(x = year, y = count, fill = locationabbr,color = locationabbr))+
  geom_line() +
  labs(
    title = "Observations number in each state from 2002 to 2010 plot",
    x = "year",
    y = "Obervation numer"
  )+
  scale_color_hue(name = " state", h = c(0,360))

```

Comments:  Since the number of states is large, each state line is difficult to be distinguished. But the tendency for most of the observation number in each state is clear, which seems stable.For example, NY always has more observation counts than other states.And, only few states like FL seems to have huge waves in 2002-2010.It seemed like FL had a weird high observation count in 2007 and 2010. Meanwhile, differences between states` observation numbers are also stable, seen from this plot.  

```{r ny_group}
# For the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State
ny_brfss = brfss %>% 
  ungroup() %>% 
  filter(year %in% c(2002,2006,2010) & locationabbr == "NY" & response == "Excellent") %>% 
  group_by(locationdesc) %>% 
  summarise(
  mean = mean(data_value, na.rm = TRUE),
  sd = sd(data_value, na.rm = TRUE) 
 ) 
  ny_brfss
 
```
Comments: During 2002,2006,2010 in NY, the Westchester County and New York County had  high "excellent" proportion with the low sd value compared with other counties. Most counties have an "excellent" proportion higher than 20 except Erie County, Bronx County and Queens County. In sd column there are 3 NA value, which is because the related county only were observed only once in specific years(2002,2006,2010).

```{r plot}

#For each year and state,compute the average proportion in each response category
tl_brfss = brfss %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(
    averange = mean(data_value)
  )
#make a five-panel plot
ggplot(tl_brfss,aes(x = year, y = averange, fill = locationabbr, color = locationabbr))+
  geom_line()+
  facet_grid(. ~ response)+
  labs(
    title = "Distribution of each response`s state-level average proportion from 2002 to 2010",
      x = "year",
      y = "proportion(%)",
      color = "States"
  )+
  theme(axis.text = element_text(size = 4),
        legend.title = element_text(size = 4),
        legend.text = element_text(size = 3)
        )
```
Comments:This plot shows (1)in each panel(response category),the differences between most state-level proportion averages keep within 10%. (2)Obvious differences between different panels, which shows a overall tendency for all states that the percentages of responses are arranged in descending order :"Very good" >"good" >"Excellent" >"Fair" >"Poor".


##problem 2

```{r p2_description_unfinished}
#load data
library(p8105.datasets)
data(instacart)
names(instacart)
#This is a 1384617 rows * 15 cols dataset which indicates the information of costomers` order details and their products` details. 
#The key variables include days_since_prior_order, aisle_id, department_id and so on.
#For example,in var 'days_since_prior_order', each value means the gap days from this costomer`s last order to this one. The var 'department_id' shows which departments each product belong to, which can be studied to explore which departments is most sold.

```

```{r p2_}
#how many aisles are there, and which aisles are the most items ordered from?
nrow(distinct(instacart, aisle))
instacart %>% 
  count(aisle) %>% 
  arrange(n) %>% 
  tail()
#Make a plot that shows the number of items ordered in each aisle.
instacart %>% 
  select(product_id,product_name,aisle_id,aisle) %>%  
  ggplot(aes(x = aisle_id))+geom_histogram(alpha = .8)+
  labs(
    title = "Histogram",
    x = "ID Number of aisles",
    y = "Number of items"
  )+
  scale_x_continuous(
    breaks = c(10,20,30,40,50,60,70,80,90,100,110,120,130,140),
    labels = c(10,20,30,40,50,60,70,80,90,100,110,120,130,140)
  )

#Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
bak_in = instacart %>% 
  select(product_id,product_name,aisle_id,aisle) %>%  
  filter(aisle == "baking ingredients") %>%  
   group_by(aisle,product_name) %>% 
   summarise( n = n()) %>% 
   arrange(n) %>% 
   tail(1)
dog_f = instacart %>% 
  select(product_id,product_name,aisle_id,aisle) %>%  
  filter(aisle == "dog food care") %>%  
   group_by(aisle,product_name) %>% 
   summarise( n = n()) %>% 
   arrange(n) %>% 
   tail(1)
pac_v = instacart %>% 
  select(product_id,product_name,aisle_id,aisle) %>%  
  filter(aisle == "packaged vegetables fruits") %>%  
   group_by(aisle,product_name) %>% 
   summarise( n = n()) %>% 
   arrange(n) %>% 
   tail(1)

#Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week;

```
Comments:
(1)There are 134 aisles, and the fresh vegetables is the most items ordered from. 
(2)The Histogram shows roughly that two aisles have obviously more items than others and their id is between 20-25 and 80-85.Apart from this, we can see the distribution of the item numbers in each aisle arranged by their id number.  

Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).



 
